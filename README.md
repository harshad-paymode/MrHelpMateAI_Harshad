# MrHelpMate

**DEMO**

https://github.com/user-attachments/assets/b3b51a80-6222-406c-b020-bd03bf1ee2f6


**Why this matters**
- Helps users understand complex insurance documents with clear, grounded answers.
-Reduces misinterpretation by highlighting relevant terms and conditions instead of guessing.
- Encourages transparency by surfacing citations directly from the policy text.

**Stack / Tech**
- LangChain (RAG + LCEL pipelines)
- ChromaDB (vector store)
- BM25 + vector retrieval + reranking
- Streamlit (UI)
- LLMs (generation + embeddings)
- Python, logging, dataclasses, guardrails

**How it works (high level)**
1. Documents are chunked and indexed offline.
2. Queries retrieve relevant chunks (BM25 + vector).
3. Results are reranked and validated.
4. Guardrails (moderation + fallback + small-talk routing) decide how to respond.
5. The LLM answers ONLY from the policy — or says it’s unavailable.

**Strengths**
- Moderation for unsafe queries
- Smart fallback instead of hallucinations
- Small-talk routing
- Threshold-based relevance filtering
- Clear citations where possible

**Evaluation Results**
The 40 evaluation questions were generated by a large language model using the policy text to create abstract, multi-hop questions requiring cross-section reasoning.

| Metric                   | Mean | Min  | Median | Max  |
| ------------------------ | ---- | ---- | ------ | ---- |
| **Contextual Precision** | 0.96 | 0.50 | 1.00   | 1.00 |
| **Contextual Recall**    | 1.00 | 1.00 | 1.00   | 1.00 |
| **Answer Relevancy**     | 0.88 | 0.45 | 0.90   | 1.00 |
| **Faithfulness**         | 0.81 | 0.00 | 0.85   | 1.00 |

**Interpretation**
MrHelpMate achieves perfect recall (1.00) and very high precision (0.96), meaning the system consistently retrieves the correct policy sections with minimal noise.
- Answer Relevancy (0.88) shows the assistant usually answers the question directly, with occasional verbosity or tangents.
- Faithfulness (0.81) shows some responses include mild over-generalization or extrapolation beyond the retrieved text.- 

**Limitations / Future Work**
- Limited conversational memory
- Performance tuning



**Run locally**
```bash
pip install -r requirements.txt
python -m ingestion.build_index      # one-time
python -m streamlit run app.py
